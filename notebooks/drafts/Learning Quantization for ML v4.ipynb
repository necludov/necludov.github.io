{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import seed as set_seed, randint\n",
    "from scipy import stats\n",
    "from jax.interpreters.xla import DeviceArray\n",
    "from jax import grad, jit\n",
    "import jax\n",
    "import pandas as pd\n",
    "from numpy import ravel_multi_index as numpy_ravel_multi_index\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rcParams['image.cmap'] = 'Dark2'\n",
    "plt.rcParams['figure.figsize'] = [9.0, 6.0]\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_hyperplane(X, w) -> '(X.shape[0],)':\n",
    "    return np.sign(X @ w)\n",
    "\n",
    "\n",
    "def plot_2d_problem(X, y=None, B=None, RX=None):\n",
    "    x = np.linspace(X.min(), X.max(), 100)\n",
    "    plt.scatter(*X.T, c=y, alpha=0.25)\n",
    "    plt.plot(x, x, 'k--', alpha=0.15)\n",
    "    if B is not None:\n",
    "        for b, i in zip(*B):\n",
    "            if i == 0:\n",
    "                plt.vlines(b, X[:, 1].min(), X[:, 1].max(),\n",
    "                           colors='k', linestyles='dashed')\n",
    "            elif i == 1:\n",
    "                plt.hlines(b, X[:, 0].min(), X[:, 0].max(),\n",
    "                           colors='k', linestyles='dashed')\n",
    "        \n",
    "    if RX is not None:\n",
    "        plt.scatter(*RX.T, s=100, marker='X', c='black')\n",
    "\n",
    "    \n",
    "def sigmoid(x) -> 'x.shape':\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def make_normal_unit_vector(d) -> '(d,)':\n",
    "    v = np.append(-1, np.ones(d-1))\n",
    "    return v/np.sqrt(v@v)\n",
    "\n",
    "\n",
    "def add_seperation(X, sep=0) -> 'X: (N, d); y: (N,)':\n",
    "    d = X.shape[1]\n",
    "    w = make_normal_unit_vector(d)\n",
    "    y = np.sign(X @ w)\n",
    "    X += sep*y[:, np.newaxis]*w\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_random_reconstruction_points(X, M, d) -> '(M, d)':\n",
    "    dist = stats.uniform(loc=X.min(), scale=X.max() - X.min())\n",
    "    return np.array(dist.rvs(size=(M, d)))\n",
    "\n",
    "\n",
    "def calc_best_rpoints(X, Qinv, RX=None) -> '(M, d)':\n",
    "    if RX is not None:\n",
    "        X = np.concatenate((X, RX), axis=0)\n",
    "        Qinv = np.concatenate((Qinv, np.arange(RX.shape[0])))\n",
    "    return np.array([X[Qinv == i].mean(axis=0) for i in range(M)])\n",
    "\n",
    "\n",
    "def calc_optimal_reconstruction_points(Q, X, y, aux, γ) -> '(M, d)':\n",
    "    flattener = aux['flattener']\n",
    "    aux1 = aux['aux1']\n",
    "    w = aux['w']\n",
    "    Q_flat = ((Q @ flattener)[:, np.newaxis] + aux1).ravel('F')\n",
    "    p = classify_with_hyperplane(X, w)\n",
    "    K = np.array([-1, 1])\n",
    "#     X = X[..., np.newaxis] + γ/4*(y[:, np.newaxis, np.newaxis] - K[np.newaxis, np.newaxis])*w[np.newaxis, np.newaxis]\n",
    "    X = X + γ/2*y[:, np.newaxis]*w[np.newaxis]*(p[:, np.newaxis] != y[:, np.newaxis])\n",
    "    avgs = np.bincount(Q_flat, weights=X.ravel('F'))/np.bincount(Q_flat)\n",
    "    avgs = avgs[~np.isnan(avgs)]\n",
    "    d = X.shape[1]\n",
    "    M = int(np.unique(avgs).shape[0]/d)\n",
    "    return avgs.reshape((M, -1), order='F')\n",
    "\n",
    "def freeze_points(X, i):\n",
    "    N, d = X.shape\n",
    "    assert i < d\n",
    "    X_repeated = X[np.newaxis].repeat(N, axis=0)\n",
    "    return X_repeated.at[:, :, i].set(X_repeated[:, :, i].T)\n",
    "\n",
    "\n",
    "def freeze_Q(Q, i, R):\n",
    "    N, d = Q.shape\n",
    "    assert i < d\n",
    "    Q_repeated = Q[np.newaxis].repeat(2**R, axis=0)\n",
    "    K = np.arange(2**R)[:, np.newaxis].repeat(N, axis=1)\n",
    "    return Q_repeated.at[:, :, i].set(K)\n",
    "\n",
    "\n",
    "def calc_distortion(X, RXs, d_axis=-1):\n",
    "    return ((X - RXs)**2).sum(axis=d_axis)\n",
    "\n",
    "\n",
    "def calc_01_loss(X, RXs, w):\n",
    "    return (classify_with_hyperplane(X, w) \n",
    "            != classify_with_hyperplane(RXs, w))\n",
    "\n",
    "\n",
    "def calc_boundary_distance_distortion(X, RXs, w):\n",
    "    y = classify_with_hyperplane(X, w)\n",
    "    yr = classify_with_hyperplane(RXs, w)\n",
    "    return (y - yr)/2*((X - RXs) @ w)\n",
    "\n",
    "\n",
    "def calc_loss(X, RX, Q, aux, γ, d_axis=-1):\n",
    "    flattener = aux['flattener']\n",
    "    w = aux['w']\n",
    "    RXs = RX[Q @ flattener]\n",
    "    distortion = calc_distortion(X, RXs)\n",
    "#     zero_one_loss = calc_01_loss(X, RXs, w)\n",
    "    zero_one_loss = calc_boundary_distance_distortion(X, RXs, w)\n",
    "    return ((1 - γ)*distortion + γ*zero_one_loss,\n",
    "            distortion,\n",
    "            zero_one_loss)\n",
    "\n",
    "\n",
    "def calc_nondistributed_q_indices(X, RX, R):\n",
    "    N, d = X.shape\n",
    "    Q_flat = ((X[:, np.newaxis] - RX)**2).sum(axis=-1).argmin(axis=-1)\n",
    "    return np.stack(np.unravel_index(Q_flat, d*[2**R]))[::-1].T\n",
    "\n",
    "\n",
    "def calc_optimal_quantization_indices_parallel(RX, Xs, Q, R, aux, γ):\n",
    "    N = Q.shape[0]\n",
    "    Qs = make_Qs(Q, R)\n",
    "    loss = calc_loss(Xs[:, np.newaxis], RX, Qs, aux, γ).mean(axis=-1)\n",
    "    return loss.argmin(axis=1)\n",
    "\n",
    "\n",
    "def calc_optimal_quantization_indices_sequential(RX, Xs, Q, R, aux, γ):\n",
    "    d = Q.shape[1]\n",
    "    for i in range(d):\n",
    "        Qsi = freeze_Q(Q, i, R)\n",
    "        Xsi = Xs[:, i]\n",
    "        (loss,\n",
    "         distortion,\n",
    "         zero_one_loss) = [l.mean(axis=-1) \n",
    "                           for l in calc_loss(Xsi[:, np.newaxis], RX, Qsi, aux, γ)]\n",
    "        Qi_new = np.nanargmin(loss, axis=1)\n",
    "        Q = Q.at[:, i].set(Qi_new)\n",
    "    return Q\n",
    "\n",
    "\n",
    "def make_Xs(X):\n",
    "    d = X.shape[1]\n",
    "    return np.stack([freeze_points(X, i) for i in range(d)], axis=1)\n",
    "\n",
    "\n",
    "def make_Qs(Q, R):\n",
    "    d = Q.shape[1]\n",
    "    return np.stack([freeze_Q(Q, i, R) for i in range(d)], axis=1)\n",
    "\n",
    "\n",
    "def make_boundaries(X, Q):\n",
    "    assert X.shape[1] == 2, 'Only implemented for d == 2'\n",
    "    i_sorted = X.argsort(axis=0)\n",
    "    K = np.array([0, 1])[np.newaxis].repeat(X.shape[0], axis=0)\n",
    "    boundary_indices = np.nonzero(np.diff(Q[i_sorted, K], axis=0))\n",
    "    return X[i_sorted, K][boundary_indices], boundary_indices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: 'global numpy seed' = 2\n",
    "set_seed(seed)\n",
    "\n",
    "N: 'sample size' = 1_000\n",
    "d: 'problem dimension' = 2\n",
    "R: 'bit rate per dimension' = 1\n",
    "X: 'sample' = (stats\n",
    "               .multivariate_normal(mean=stats.uniform().rvs(20*d),\n",
    "                                    cov=1e-2)\n",
    "               .rvs(size=int(N/20))\n",
    "               .reshape(-1, d))\n",
    "# X = stats.uniform().rvs((N, d))\n",
    "\n",
    "X, y = add_seperation(X, 0.05)\n",
    "M: 'number of reconstruction points' = 2**(R*d)\n",
    "RX: 'reconstruction points' = generate_random_reconstruction_points(X, M, d)\n",
    "Q: 'quantization indices' = np.array(stats.randint(0, 2**R).rvs((N, d)))\n",
    "# Q  = calc_nondistributed_q_indices(X, RX)\n",
    "flattener: 'vector that flattens the quantion indices with Q @ flattener' = np.array([2**(R*i) for i in range(d)])\n",
    "w = make_normal_unit_vector(d)\n",
    "aux1 = np.array([i*M for i in range(d)])[np.newaxis]\n",
    "aux = {'flattener': flattener,\n",
    "       'aux1': aux1,\n",
    "       'w': w}\n",
    "\n",
    "assert Q.shape == (N, d)\n",
    "\n",
    "# RX = calc_optimal_reconstruction_points(Q, X, aux)\n",
    "if d == 2:\n",
    "    plot_2d_problem(X, Q @ flattener, RX=RX)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distortion-only Optimal Quantizer Design (no side-information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs: '(repetee index, q dim, N, d)' = make_Xs(X)\n",
    "Qs: '(q indice, q dim, N, d)' = make_Qs(Q, R)\n",
    "\n",
    "# check dimensions\n",
    "assert Xs.shape == (N, d, N, d)\n",
    "assert Qs.shape == (2**R, d, N, d)\n",
    "\n",
    "# check if elements are correctly repeated\n",
    "assert all((Xs[n, i, :, i] == X[n, i]).all()\n",
    "           for i in range(d)\n",
    "           for n in range(N))\n",
    "\n",
    "assert all((Qs[k, i, :, i] == k).all()\n",
    "           for i in range(d)\n",
    "           for k in range(2**R))\n",
    "\n",
    "# check that nothing else has changed\n",
    "assert all((Xs == X)[:, i][..., [j for j in range(d) if j != i]].all()\n",
    "           for i in range(d))\n",
    "assert all((Qs == Q)[:, i][..., [j for j in range(d) if j != i]].all()\n",
    "           for i in range(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "from functools import lru_cache\n",
    "from toolz import concat\n",
    "\n",
    "memory = Memory('cache')\n",
    "\n",
    "@lru_cache()\n",
    "@memory.cache\n",
    "def run_single(d, R, N, sep=0, q_method='sequential',\n",
    "               seed=0, iters=10, γ=0, return_arrays=False):\n",
    "    set_seed(seed)\n",
    "    X = (stats\n",
    "         .multivariate_normal(mean=stats.uniform().rvs(20*d),\n",
    "                              cov=1e-2)\n",
    "         .rvs(size=int(N/20))\n",
    "         .reshape(-1, d))\n",
    "    X, y = add_seperation(X, sep)\n",
    "    M = 2**(R*d)\n",
    "    RX = generate_random_reconstruction_points(X, M, d)\n",
    "    Q  = calc_nondistributed_q_indices(X, RX, R)\n",
    "    flattener = np.array([2**(R*i) for i in range(d)])\n",
    "    aux1 = np.array([i*M for i in range(d)])[np.newaxis]\n",
    "    w = make_normal_unit_vector(d)\n",
    "    aux = {'flattener': flattener,\n",
    "           'aux1': aux1,\n",
    "           'w': w}\n",
    "    \n",
    "    if q_method == 'sequential':\n",
    "        calc_Q_func = calc_optimal_quantization_indices_sequential\n",
    "    elif q_method == 'parallel':\n",
    "        calc_Q_func = calc_optimal_quantization_indices_parallel\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    \n",
    "    Xs = make_Xs(X)\n",
    "    metrics = list()\n",
    "    for i in range(iters):\n",
    "        RX = calc_optimal_reconstruction_points(Q, X, y, aux, γ)\n",
    "        Q = calc_Q_func(RX, Xs, Q, R, aux, γ)\n",
    "        loss, distortion, zero_one_loss = calc_loss(X, RX, Q, aux, γ)\n",
    "        metrics.append({\n",
    "            'i': i,\n",
    "            'iters': iters,\n",
    "            'q_method': q_method,\n",
    "            'loss': loss.mean().item(),\n",
    "            'distortion': distortion.mean().item(),\n",
    "            'zero_one_loss': zero_one_loss.mean().item(),\n",
    "            'd': d,\n",
    "            'N': N,\n",
    "            'R': R,\n",
    "            'sep': sep,\n",
    "            'seed': seed,\n",
    "            'M': M,\n",
    "            'M_final': RX.shape[0],\n",
    "            'Q': Q if return_arrays else None,\n",
    "            'X': X if return_arrays else None,\n",
    "            'RX': RX if return_arrays else None,\n",
    "            'y': y if return_arrays else None,\n",
    "            'flattener': flattener if return_arrays else None,\n",
    "            'w': w if return_arrays else None,\n",
    "            'γ': γ,\n",
    "            'id': f'{d}:{R}:{N}:{sep}:{seed}:{iters}:{γ}'\n",
    "        })\n",
    "    return metrics\n",
    "\n",
    "results = [run_single(d, R, N, sep, q_method, seed, iters=20, γ=γ)\n",
    "           for d in [1, 2, 3]\n",
    "           for R in [1, 2]\n",
    "           for N in [500]\n",
    "           for sep in [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "           for q_method in ['sequential']\n",
    "           for seed in [0, 1, 2]\n",
    "           for γ in [0, 0.25, 0.50, 0.75, 1.0]]\n",
    "\n",
    "results_df = (pd.DataFrame(concat(results))\n",
    "                .set_index(['id', 'i'])\n",
    "                .assign(has_constant_rate=lambda df: df.groupby('id')['M_final'].transform('nunique') == 1)\n",
    "                .sort_index())\n",
    "\n",
    "results_df = (results_df\n",
    " .groupby('id')['loss']\n",
    " .apply(lambda s: (s.diff().dropna() <= 0).all())\n",
    " [lambda s: s]\n",
    " .pipe(lambda s: results_df.assign(has_nonincreasing_loss = lambda df: df.index.get_level_values(0).isin(s.index.get_level_values(0)))))\n",
    "\n",
    "results_df.loc['1:1:500:0.1:0:20:0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_df\n",
    " .reset_index()\n",
    " .drop_duplicates('id')\n",
    " ['has_constant_rate']\n",
    " .agg(['mean', 'sum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_df\n",
    " .reset_index()\n",
    " .drop_duplicates('id')\n",
    " .pipe(lambda df: pd.crosstab(df['M'], df['has_constant_rate'], margins=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-monotonic cases (need to investigate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_df\n",
    " .reset_index()\n",
    " .drop_duplicates('id')\n",
    " .pipe(lambda df: pd.crosstab(df['has_nonincreasing_loss'], df['has_constant_rate'], margins=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_df\n",
    " .query(\"~has_nonincreasing_loss & has_constant_rate & d == 2\")\n",
    " .head(60)\n",
    " .tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "R = 1\n",
    "N = 500\n",
    "sep = 0.1\n",
    "seed = 0\n",
    "iters = 20\n",
    "γ = 0.5\n",
    "results = run_single(d=d, R=R, N=N, sep=sep, seed=seed,\n",
    "               return_arrays=True, iters=iters, γ=γ)\n",
    "r = results[-1]\n",
    "zero_one_loss = (r['y'] != classify_with_hyperplane(r['RX'][r['Q'] @ r['flattener']], r['w'])).mean()\n",
    "B = make_boundaries(r['X'], r['Q'])\n",
    "# plt.figure(figsize=(20,10))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "plot_2d_problem(r['X'], r['Q'] @ r['flattener'], B, RX=r['RX'])\n",
    "\n",
    "# ax = plt.subplot(1, 2, 2)\n",
    "(pd.DataFrame([{k:v for k,v in r.items() \n",
    "               if k in ['loss', 'distortion', 'zero_one_loss']}\n",
    "               for r in results])\n",
    "   [2:]\n",
    "   .plot(style='o--', subplots=True))\n",
    "\n",
    "print(f'''\n",
    "γ = {γ}\n",
    "sep = {sep}\n",
    "R = {R}\n",
    "N = {N}\n",
    "total loss: {r[\"loss\"]}\n",
    "01-loss: {zero_one_loss}\n",
    "distortion: {r[\"distortion\"]}\n",
    "''')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ = 0\n",
    "sep = 0.1\n",
    "seed = 2292\n",
    "R = 1\n",
    "N = 2_000\n",
    "r = run_single(d=2, R=R, N=N, sep=sep, seed=seed,\n",
    "               return_arrays=True, iters=20, γ=γ)[-1]\n",
    "zero_one_loss = (r['y'] != classify_with_hyperplane(r['RX'][r['Q'] @ r['flattener']], r['w'])).mean()\n",
    "B = make_boundaries(r['X'], r['Q'])\n",
    "plot_2d_problem(r['X'], r['Q'] @ r['flattener'], B, RX=r['RX'])\n",
    "plt.show()\n",
    "print(f'''\n",
    "γ = {γ}\n",
    "sep = {sep}\n",
    "R = {R}\n",
    "N = {N}\n",
    "total loss: {r[\"loss\"]}\n",
    "01-loss: {zero_one_loss}\n",
    "distortion: {r[\"distortion\"]}\n",
    "''')\n",
    "\n",
    "γ = 0.8\n",
    "sep = 0\n",
    "seed = 2292\n",
    "R = 1\n",
    "N = 2_000\n",
    "r = run_single(d=2, R=R, N=N, sep=sep, seed=seed,\n",
    "               return_arrays=True, iters=20, γ=γ)[-1]\n",
    "zero_one_loss = (r['y'] != classify_with_hyperplane(r['RX'][r['Q'] @ r['flattener']], r['w'])).mean()\n",
    "B = make_boundaries(r['X'], r['Q'])\n",
    "plot_2d_problem(r['X'], r['Q'] @ r['flattener'], B, RX=r['RX'])\n",
    "plt.show()\n",
    "print(f'''\n",
    "γ = {γ}\n",
    "sep = {sep}\n",
    "R = {R}\n",
    "N = {N}\n",
    "total loss: {r[\"loss\"]}\n",
    "01-loss: {zero_one_loss}\n",
    "distortion: {r[\"distortion\"]}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (results_df\n",
    "      .query(\"has_nonincreasing_loss\")\n",
    "      .query(\"has_constant_rate\")\n",
    "      .query(\"i + 1 == 20\")\n",
    "      .query(\"seed == 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the relationship between $\\gamma$ and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [run_single(d=d, R=1, N=500, sep=0, iters=30, γ=γ, seed=5432)\n",
    "           for γ in np.linspace(0, 1, 20)\n",
    "           for d in [3, 4, 5]]\n",
    "\n",
    "results_df = pd.DataFrame(concat(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://web.stanford.edu/~bgirod/pdfs/RebolloDCC03.pdf\n",
    "2. https://jakevdp.github.io/blog/2017/03/22/group-by-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The loss (distortion) for `calc_optimal_quantization_indices_sequential` is non-increasing as long as the number of reconstruction points doesn't change during optimization. I'm not quite sure how to handle rate changes during optimization.\n",
    "2. It looks like $0$ separation implies that $\\gamma^\\star = 0$\n",
    "3. Compare with optimal OnTheLine quantizer.\n",
    "4. What is the sample complexity?\n",
    "5. Add rate/entropy constraint.\n",
    "6. Does the new distortion function have the same properties as the original in [1] (i.e. $\\mathbb{E}j_i = \\mathcal{J}$ for all quantizers $i$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsevero.com] *",
   "language": "python",
   "name": "conda-env-dsevero.com-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
